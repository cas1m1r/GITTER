======================: FILES :======================
======================: README CONTENT :======================
<div align="center">
<img src="./images/logo.png" width="300px">

**The pure and clear PyTorch Distributed Training Framework.**

</div>

* [Introduction](#introduction)
* [Requirements and Usage](#requirements-and-usage)
  * [Dependency](#dependency)
  * [Dataset](#dataset)
  * [Basic Usage](#basic-usage)
  * [Slurm Cluster Usage](#slurm-cluster-usage)
* [Baselines](#baselines)
* [Zombie processes problem](#zombie-processes-problem)
* [Acknowledgments](#acknowledgments)
* [Citation](#citation)

## Introduction


Distribuuuu is a Distributed Classification Training Framework powered by native PyTorch.

Please check [tutorial](./tutorial/) for detailed **Distributed Training** tutorials:

- Single Node Single GPU Card Training [[snsc.py](./tutorial/snsc.py)]
- Single Node Multi-GPU Cards Training (with DataParallel) [[snmc_dp.py](./tutorial/snmc_dp.py)]
- Multiple Nodes Multi-GPU Cards Training (with DistributedDataParallel)
    - torch.distributed.launch [[mnmc_ddp_launch.py](./tutorial/mnmc_ddp_launch.py)]
    - torch.multiprocessing [[mnmc_ddp_mp.py](./tutorial/mnmc_ddp_mp.py)]
    - Slurm Workload Manager [[mnmc_ddp_slurm.py](./tutorial/mnmc_ddp_slurm.py)]
- ImageNet training example [[imagenet.py](./tutorial/imagenet.py)]

For the complete training framework, please see [distribuuuu](./distribuuuu/). 

## Requirements and Usage

### Dependency

- Install **PyTorch>= 1.6** (has been tested on **1.6, 1.7.1, 1.8** and **1.8.1**)
- Install other dependencies: ``pip install -r requirements.txt``

### Dataset

Download the ImageNet dataset and move validation images to labeled subfolders, using the script [valprep.sh](https://raw.githubusercontent.com/soumith/imagenetloader.torch/master/valprep.sh). 



<details>
  <summary>Expected datasets structure for ILSVRC</summary>

``` 
ILSVRC
|_ train
|  |_ n01440764
|  |_ ...
|  |_ n15075141
|_ val
|  |_ n01440764
|  |_ ...
|  |_ n15075141
|_ ...
```

Create a directory containing symlinks:

``` bash
mkdir -p /path/to/distribuuuu/data
```

Symlink ILSVRC:

``` bash
ln -s /path/to/ILSVRC /path/to/distribuuuu/data/ILSVRC
```

</details>



### Basic Usage

Single Node with one task


``` bash
# 1 node, 8 GPUs
python -m torch.distributed.launch \
    --nproc_per_node=8 \
    --nnodes=1 \
    --node_rank=0 \
    --master_addr=localhost \
    --master_port=29500 \
    train_net.py --cfg config/resnet18.yaml
```

Distribuuuu use [yacs](https://github.com/rbgirshick/yacs), a elegant and lightweight package to define and manage system configurations.
You can setup config via a yaml file, and overwrite by other opts. If the yaml is not provided, the default configuration file will be used, please check [distribuuuu/config.py](./distribuuuu/config.py).

```bash
python -m torch.distributed.launch \
    --nproc_per_node=8 \
    --nnodes=1 \
    --node_rank=0 \
    --master_addr=localhost \
    --master_port=29500 \
    train_net.py --cfg config/resnet18.yaml \
    OUT_DIR /tmp \
    MODEL.SYNCBN True \
    TRAIN.BATCH_SIZE 256

# --cfg config/resnet18.yaml parse config from file
# OUT_DIR /tmp            overwrite OUT_DIR
# MODEL.SYNCBN True       overwrite MODEL.SYNCBN
# TRAIN.BATCH_SIZE 256    overwrite TRAIN.BATCH_SIZE
```


<details>
  <summary>Single Node with two tasks</summary>


```bash
# 1 node, 2 task, 4 GPUs per task (8GPUs)
# task 1:
CUDA_VISIBLE_DEVICES=0,1,2,3 python -m torch.distributed.launch \
    --nproc_per_node=4 \
    --nnodes=2 \
    --node_rank=0 \
    --master_addr=localhost \
    --master_port=29500 \
    train_net.py --cfg config/resnet18.yaml

# task 2:
CUDA_VISIBLE_DEVICES=4,5,6,7 python -m torch.distributed.launch \
    --nproc_per_node=4 \
    --nnodes=2 \
    --node_rank=1 \
    --master_addr=localhost \
    --master_port=29500 \
    train_net.py --cfg config/resnet18.yaml
```

</details>

<details>
  <summary>Multiple Nodes Training</summary>

```bash
# 2 node, 8 GPUs per node (16GPUs)
# node 1:
python -m torch.distributed.launch \
    --nproc_per_node=8 \
    --nnodes=2 \
    --node_rank=0 \
    --master_addr="10.198.189.10" \
    --master_port=29500 \
    train_net.py --cfg config/resnet18.yaml

# node 2:
python -m torch.distributed.launch \
    --nproc_per_node=8 \
    --nnodes=2 \
    --node_rank=1 \
    --master_addr="10.198.189.10" \
    --master_port=29500 \
    train_net.py --cfg config/resnet18.yaml
```

</details>

### Slurm Cluster Usage

```bash
# see srun --help 
# and https://slurm.schedmd.com/ for details

# example: 64 GPUs
# batch size = 64 * 128 = 8192
# itertaion = 128k / 8192 = 156 
# lr = 64 * 0.1 = 6.4

srun --partition=openai-a100 \
     -n 64 \
     --gres=gpu:8 \
     --ntasks-per-node=8 \
     --job-name=Distribuuuu \
     python -u train_net.py --cfg config/resnet18.yaml \
     TRAIN.BATCH_SIZE 128 \
     OUT_DIR ./resnet18_8192bs \
     OPTIM.BASE_LR 6.4
```

## Baselines

**Baseline** models trained **by Distribuuuu**:

- We use SGD with momentum of 0.9, a half-period **cosine schedule**, and train for **100** epochs.
- We use a **reference learning rate** of **0.1** and a weight decay of **5e-5** (1e-5 For EfficientNet).
- The actual learning rate(**Base LR**) for each model is computed as **(batch-size / 128) * reference-lr**.
- Only standard data augmentation techniques(RandomResizedCrop and RandomHorizontalFlip) are used.

**PS: use other robust tricks(more epochs, efficient data augmentation, etc.) to get better performance.**


|                     Arch                     | Params(M) |    Total batch     | Base LR | Acc@1  | Acc@5  |                                                           model / config                                                           |
| :------------------------------------------: | :-------: | :----------------: | :-----: | :----: | :----: | :--------------------------------------------------------------------------------------------------------------------------------: |
|                   resnet18                   |  11.690   |   256 (32*8GPUs)   |   0.2   | 70.902 | 89.894 |    [Drive](https://drive.google.com/file/d/18a6QFc_DoTHo3TWkN_EsptyGmhF97sVw/view?usp=sharing) / [cfg](./config/resnet18.yaml)     |
|                   resnet18                   |  11.690   |  1024 (128*8GPUs)  |   0.8   | 70.994 | 89.892 |                                                                                                                                    |
|                   resnet18                   |  11.690   | 8192 (128*64GPUs)  |   6.4   | 70.165 | 89.374 |                                                                                                                                    |
|                   resnet18                   |  11.690   | 16384 (256*64GPUs) |  12.8   | 68.766 | 88.381 |                                                                                                                                    |
|               efficientnet_b0                |   5.289   |   512 (64*8GPUs)   |   0.4   | 74.540 | 91.744 | [Drive](https://drive.google.com/file/d/1nSLQBBRKnAJYdoFhUUVsV8qI5270ooq3/view?usp=sharing) / [cfg](./config/efficientnet_b0.yaml) |
|                   resnet50                   |  25.557   |   256 (32*8GPUs)   |   0.2   | 77.252 | 93.430 |    [Drive](https://drive.google.com/file/d/1rUY1mSYTxe7jWzzcWrreg398tbSNXtnv/view?usp=sharing) / [cfg](./config/resnet50.yaml)     |
| [botnet50](https://arxiv.org/abs/2101.11605) |  20.859   |   256 (32*8GPUs)   |   0.2   | 77.604 | 93.682 |    [Drive](https://drive.google.com/file/d/1-jvhJaMyy-KziAuFnmt5rkoZrm5364UF/view?usp=sharing) / [cfg](./config/botnet50.yaml)     |
|                 regnetx_160                  |  54.279   |   512 (64*8GPUs)   |   0.4   | 79.992 | 95.118 |   [Drive](https://drive.google.com/file/d/1w2LtMKiLHwz27fJOmymQmPCX1yPDuPsm/view?usp=sharing) / [cfg](./config/regnetx_160.yaml)   |
|                 regnety_160                  |  83.590   |   512 (64*8GPUs)   |   0.4   | 80.598 | 95.090 |   [Drive](https://drive.google.com/file/d/1dmD94jeZCaYLI9DhbMN0V1uG6_KHkx_o/view?usp=sharing) / [cfg](./config/regnety_160.yaml)   |
|                 regnety_320                  |  145.047  |   512 (64*8GPUs)   |   0.4   | 80.824 | 95.276 |   [Drive](https://drive.google.com/file/d/1pVbSy4YSlWBra1C2NLTNwJkk_zOTomZg/view?usp=sharing) / [cfg](./config/regnety_320.yaml)   |
## Zombie processes problem


Before PyTorch1.8, ``torch.distributed.launch`` will leave some zombie processes after using  ``Ctrl`` + ``C``, try to use the following cmd to kill the zombie processes. ([fairseq/issues/487](https://github.com/pytorch/fairseq/issues/487)):

```bash
kill $(ps aux | grep YOUR_SCRIPT.py | grep -v grep | awk '{print $2}')
```

PyTorch >= 1.8 is suggested, which fixed the issue about zombie process. ([pytorch/pull/49305](https://github.com/pytorch/pytorch/pull/49305))


## Acknowledgments

Provided codes were adapted from:

- [facebookresearch/pycls](https://github.com/facebookresearch/pycls)
- [pytorch/examples](https://github.com/pytorch/examples/)
- [open-mmlab/mmcv](https://github.com/open-mmlab/mmcv)

I strongly recommend you to choose [pycls](https://github.com/facebookresearch/pycls), a brilliant image classification codebase and adopted by a number of projects at [Facebook AI Research](https://github.com/facebookresearch).



## Citation

```
@misc{bigballon2021distribuuuu,
  author = {Wei Li},
  title = {Distribuuuu: The pure and clear PyTorch Distributed Training Framework},
  howpublished = {\url{https://github.com/BIGBALLON/distribuuuu}},
  year = {2021}
}
```

Feel free to contact me if you have any suggestions or questions, issues are welcome,
create a PR if you find any bugs or you want to contribute. :cake:
====================== GIT HISTORY: ======================
eaaa922 HEAD@{0}: clone: from https://github.com/BIGBALLON/distribuuuu
commit eaaa9229ab66a1c5f24c2a07aced21adf9af7895
Author: WILL LEE <fm.bigballon@gmail.com>
Date:   Fri May 7 21:04:23 2021 +0800

    [feat] Add eta time (#20)

commit 4d62aa06c7b6ae1c5deae43e4a4cc311627e69b2
Author: WILL LEE <fm.bigballon@gmail.com>
Date:   Thu May 6 00:01:08 2021 +0800

    [feat] Add more baselines (#19)

commit 556e04ac3945283388d6b19aeab9ff6592118d8a
Author: WILL LEE <fm.bigballon@gmail.com>
Date:   Thu Apr 29 07:43:34 2021 -0500

    [feat] Add count parameters (#18)

commit 5a4a89ee2a4dffcd3a996329ce94af1e6529dbbb
Author: WILL LEE <fm.bigballon@gmail.com>
Date:   Fri Apr 9 07:00:06 2021 -0500

    [feat] Add robust cfg & add timm (#17)

commit 3e46d0153509a6dd2618b40557f979dda45f31c5
Author: WILL LEE <fm.bigballon@gmail.com>
Date:   Thu Apr 8 02:08:05 2021 -0500

    [fixed] Add item() for device error (#13)

commit 88f9aecae347f9ae5b1d94a5e7e6160ac4a61374
Author: WILL LEE <fm.bigballon@gmail.com>
Date:   Fri Mar 26 21:30:00 2021 +0800

    [feat] Remove distributed DP module (#11)

commit 5f18fb5648861c788ffafdd8c5ea96a59e29a8e6
Author: WILL LEE <fm.bigballon@gmail.com>
Date:   Wed Mar 24 14:59:21 2021 +0800

    [fixed] Support weight != height (#10)

commit 50302c6f83cd427f6a57e7797313f343a0cb4a63
Author: WILL LEE <fm.bigballon@gmail.com>
Date:   Thu Mar 11 22:29:47 2021 +0800

    [docs] Add readme & fixed some cfgs (#8)

commit 9b299ad3f65ab986e67006446466888d7d3f48d6
Author: WILL LEE <fm.bigballon@gmail.com>
Date:   Mon Mar 8 20:58:52 2021 +0800

    [feat] Add BoTNet (#7)

commit 9e6f55b0309ba0f89bc1bb5c3ff35a75171cf6c0
Author: WILL LEE <fm.bigballon@gmail.com>
Date:   Sun Mar 7 19:35:09 2021 +0800

    [fixed] Change weight decay & add deterministic for reproduce the experiment (#6)

commit c99028962f57b05b341e34684662dbd4ccef9fdd
Author: WILL LEE <fm.bigballon@gmail.com>
Date:   Fri Mar 5 17:08:58 2021 +0800

    [improve] Refactor & add test_net (#5)

commit 54a6eb1bf0326ae1ab51874d2107b5448f47b12a
Author: WILL LEE <fm.bigballon@gmail.com>
Date:   Thu Mar 4 18:31:11 2021 +0800

    [fixed] cfg and opts (#4)
    
    * update readme.md
    
    * fixed cfg and opts
    
    * fixed epoch

commit e7ebad8d6500732369c1356fac3ab3a9d578ba04
Author: WILL LEE <fm.bigballon@gmail.com>
Date:   Thu Mar 4 18:10:40 2021 +0800

    [fixed] Various improvement (#3)

commit 108d966f225071fbb7ee3c8924b5cad62a954f61
Author: WILL LEE <fm.bigballon@gmail.com>
Date:   Wed Mar 3 23:50:39 2021 +0800

    [feat] Add imagenet benchmark (#2)

commit bdd7dc54ccfea1abaa354ae8cd8285148f3a5f3f
Author: WILL LEE <fm.bigballon@gmail.com>
Date:   Tue Mar 2 21:44:48 2021 +0800

    [feat] Add imagenet training (#1)

commit 704e7aff697a02e963dab8dc309dcbf558845031
Author: bigballon <fm.bigballon@gmail.com>
Date:   Sun Dec 20 23:15:17 2020 +0800

    [feat] Add mnmc_ddp_mp.py

commit 51cf49edcefba6400017f776a39a1e8aca3915b2
Author: bigballon <fm.bigballon@gmail.com>
Date:   Sat Dec 19 18:05:23 2020 +0800

    [fix] Update res18 and fixed sampler bug

commit 862184d1859d066c4fced75bfbaa10fc7be9d6e6
Author: bigballon <fm.bigballon@gmail.com>
Date:   Fri Dec 18 22:40:12 2020 +0800

    [feat] Add DDP

commit ecec5539325f14d3a1fc70d8122fe4b7f844e3f8
Author: Wei Li <fm.bigballon@gmail.com>
Date:   Fri Dec 18 17:13:56 2020 +0800

    [feat] Add smsc and smmc_dp
